% Created 2023-04-25 Tue 21:50
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{cancel}
\author{Lokesh Mohanty}
\date{\today}
\title{Reinforcement Learning}
\hypersetup{
 pdfauthor={Lokesh Mohanty},
 pdftitle={Reinforcement Learning},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.0.60 (Org mode 9.7-pre)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\section{References:}
\label{sec:orgba8d025}
\subsection{Reinforcement Learning: An Introduction (Sutton, Barto)}
\label{sec:org7fdb8dc}
\begin{itemize}
\item Introduction to Reinforcement Learning
\item Multi-armed bandits
\end{itemize}

\subsection{Neuro-Dynamic Programming (Bertsekas, Tsitsiklis)}
\label{sec:orgc0d20be}
\begin{itemize}
\item Finite Horizon Problem
\item Stochastic Shortest Path Problems (Study)
\end{itemize}

\subsection{Dynamic Programming and Optimal Control (Bertsekas)}
\label{sec:org215cadb}
\begin{itemize}
\item Stochastic Shortest Path Problems (Practice problems)
\end{itemize}

\section{Doubts}
\label{sec:org80f7640}
\begin{itemize}
\item How does exploration happen in greedy multi-armed bandits
\item Upper confidence bound
\end{itemize}

\section{Temporal difference Algorithm (TD(\(\lambda\)))}
\label{sec:org0835c3a}

Consider the (l+1) step Bellman equation

\begin{align*}
  J_{\pi}(i_k) = E_{\pi} \left[ \sum_{n=0}^l g(i_k, i_{k+1}) + J_{\pi}(i_{k+l+1}) \right] \text{, (assuming $\lambda = 1$)}
\end{align*}

Since l is arbitrary, we form a weighted average of these Bellman equations

Let \(0 \le \lambda < 1\), Since \((1 - \lambda) \sum_{l=0}^{\infty} \lambda^l = 1\),
we rewrite the above to obtain a weighted Bellman equation

\begin{align*}
  J_{\pi}(i_k) &= (1 - \lambda) E \left[ \sum_{l=0}^{\infty} \lambda^l \left( \sum_{m=0}^l g(i_{k + m}, i_{k+m+1}) + J_{\pi}(i_{k+l+1}) \right) \right] \\
               &= (1 - \lambda) E \left[ \sum_{l=0}^{\infty} \lambda^l \sum_{m=0}^l g(i_{k+m}, i_{k + m + 1}) \right]
                + (1 - \lambda) E \left[ \sum_{l=0}^{\infty} \lambda^l J_{\pi}(i_{k+l+1}) \right]
\end{align*}

Expanding the 1st part
\begin{align*}
(1 - \lambda) E \left[ \sum_{l=0}^{\infty} \lambda^l \sum_{m=0}^l g(i_{k+m}, i_{k + m + 1}) \right] &=
    (1 - \lambda) E \left[ \sum_{m=0}^{\infty} \sum_{l=m}^l \lambda^l g(i_{k+m}, i_{k + m + 1}) \right] \\
 &= (1 - \lambda) \frac{E \left[ \sum_{m=0}^{\infty} \lambda^m g(i_{k + m}, i_{k+m+1}) \right]}{(1 - \lambda)} \\
 &= E \left[ \sum_{m=0}^{\infty} \lambda^m g(i_{k+m}, i_{k+m+1}) \right]
\end{align*}

Expanding the 2nd part

\begin{align*}
(1 - \lambda) E \left[ \sum_{l=0}^{\infty} \lambda^l J_{\pi}(i_{k+l+1}) \right] &=
    E \left[ \sum_{l=0}^{\infty}(\lambda^l - \lambda^{l+1}) J_{\pi}(i_{k+l+1}) \right] \\
 &= E \left[ (1 - \lambda) J_{\pi}(i_{k+1}) + (\lambda - \lambda^2) J_{\pi}(i_{k+2}) + ... \right] \\
 &= E \left[ J_{\pi}(i_{k+1}) - J_{\pi}(i_k) + \lambda (J_{\pi}(i_{k+2}) - J_{\pi}(i_{k+1})) + \lambda^2 (J_{\pi}(i_{k+3}) - J_{\pi}(i_{k+2})) + ... + J_{\pi}(i_k) \right]) \\
 &= E \left[ \sum_{m=0}^{\infty} \lambda^m (J_{\pi}(i_{k+m+1}) - J_{\pi}(i_{k+m})) \right] + J_{\pi}(i_k)
\end{align*}
Combining the 2 parts, we get

\begin{align*}
 J_{\pi}(i_k) = E \left[ \sum^{\infty}_{m=0} \lambda^m \left( g(i_{k+m}, i_{k+m+1}) + J_{\pi}(i_{k+m+1}) - J_{\pi}(i_{k+m}) \right) \right] + J_{\pi}(i_k) \\
\end{align*}
Since we are in the setting of SSPP, there is a time N with \(N < \infty\) such that \(i_N = 0\) (terminal state).
Further, \(v_{\pi}(i_N) = 0\), \(g(i_{N+m}, i_{N+m}) = 0\) \(\forall m \ge 0\).

Let \(d_m = g(i_m, i_{m+1}) + J_{\pi}(i_{m+1}) - J_{\pi}(i_m)\) (temporal difference term)

Then,

\begin{align*}
  J_{\pi}(i_k) &= E \left[ \sum_{m=0}^{\infty} \lambda^m d_{m+k} \right] + J_{\pi}(i_k) \\
               &= E \left[ \sum_{m=k}^{\infty} \lambda^{m-k} d_m \right] + J_{\pi}(i_k) \\
E \left[ \sum_{m=k}^{\infty} \lambda^{m-k} d_m \right] &= 0 \text{, (true since $E_{\pi}[d_m] = 0, \forall m$)}
\end{align*}

\subsection{Robbins Monro Algorithm (for the above)}
\label{sec:orgddbace0}

\begin{align*}
  J(i_k) :&= J(i_k) + Y \sum_{m=k}^{\infty} \lambda^{m-k}\overline{d}_m \\
\text{where } \overline{d}_m &= g(i_m, i_{m+1}) + J(i_{m+1}) - J(i_m)
\end{align*}
Here, \(Y\) is the step-size parameter
As the number of iterates tends to \(\infty\),
\begin{align*}
  J(i_k) \rightarrow J_{\pi}(i_k)
\end{align*}
\subsection{Special Cases}
\label{sec:org8d195c0}
\begin{enumerate}
\item \(\lambda = 0\) (TD(0) algorithm)
\end{enumerate}
\begin{align*}
  J(i_k) :&= J(i_k) + Y \overline{d}_k \\
          &= J(i_k) + Y (g(i_k, i_{k+1}) + J(i_{k+1}) - J(i_k))
\end{align*}
\begin{enumerate}
\item \(\lambda = 1\) (Monte-Carlo or TD(1) algorithm)
\end{enumerate}
\begin{align*}
  J(i_k) :&= J(i_k) + Y \sum_{m=k}^{N-1}\overline{d}_k \\
          &= J(i_k) + Y (\overline{d}_k + \overline{d}_{k+1} + ... + \overline{d}_{N-1}) \\
          &= J(i_k) + Y (g(i_k, i_{k+1}) + g(i_{k+1}, i_{k+2}) + ... + g(i_{N-1}, i_N) + J(i_{k+1}) - J(i_k)) \\
\implies J(i_k) :&= J(i_k) + Y (g(i_k, i_{k+1}) + g(i_{k+1}, i_{k+2}) + ... + g(i_{N-1}, i_N) + J(i_{k+1}) - J(i_k))
\end{align*}

\subsection{Q-learning}
\label{sec:org0939c3e}
Recall now the Bellman equation for optimality
\begin{align*}
  J^{*}(i) &= \min_{\mu \in A(i)} \sum_{j \in S} p_{ij}(\mu) (g(i, \mu, j) + J^{*}(j)) \text{, $i \in S$ (SSPP setting)} \\
  \text{Let } Q^{*}(i, \mu) &= \sum_{j \in S} p_{ij}(\mu) (g(i, \mu, j) + J^{*}(j)) \text{, $i \in S$, (these are called Q-values)} \\
\end{align*}
Then,
\begin{align*}
 J^{*}(i) &= \min_{\mu \in A(i)} Q^{*}(i, \mu) \text{, $\forall i \in S$}
\end{align*}
Thus, (Q-Bellman Equation in the state-action tuples (i, \(\mu\)))
\begin{align*}
  Q^{*}(i, \mu) &= \sum_{j \in S} p_{ij}(\mu) (g(i, \mu, j) + \min_{\mu \in A(j)} Q^{*}(j, \mu)
                 = E \left[ g(i, \mu, n) + \min_{\mu \in A(n)} Q^{*}(n, \mu) \right]
\end{align*}

Numerical procedure for solving Q-Bellman Equation

Q-value iteration:
\begin{align*}
Q_{m+1}(i, \mu) = \sum_{j \in S} p_{ij}(\mu) (g(i, \mu, j) + \min_{\mu \in A(j)}Q_m(j, \mu)) \text{, $m = 0, 1, 2, ...$}
\end{align*}

In case we don't know \(p_{ij}(\mu)\), we resent to data driven (model-free) scheme. (update full Q-table at each instant)
\begin{align*}
 Q_{m+1}(i, \mu) = Q_{m(i, \mu))} + Y (g(i, \mu, j) + \min_{\mu \in A(j)} Q_m(j, \mu) - Q_m(i, \mu))
\end{align*}
\textbf{Key problem}:
When Q-estimates are not properly developed, there is significant bias in algorithm.
This algorithm requires one to explore Q-values sufficiently for the various actions.

Consider asynchronous version of the algorithm
\begin{align*}
Q_{m+1}(i_m, \mu_m) = Q_m(i_m, \mu_m) + Y(i_m, \mu_m) (g(i_m, \mu_m, i_{m+1}) + Q_m(i_{m+1}, \mu_{m+1}) - Q_m(i_m, \mu_m))
\end{align*}
Here \(Y(i_m, \mu_m) = \frac{1}{m}\) if \(i_m\) is the state visited at m and \(\mu_m\)

\textbf{Note:} if \(\mu_m\) is selected according to some policy \(\pi\)(fixed) in \(i_m\), then TD(1) is simply TD(0)

\subsubsection{Recall the Q-learning algorithm}
\label{sec:org08e2017}
\begin{align*}
Q_{t+1}(i_t, \mu_t) = Q_t(i_t, \mu_t) + \gamma (g(i_t, \mu_t, i_{t+1}) + Q_t(i_{t+1}, \mu_{t+1}) - Q_t(i_t, \mu_t))
\end{align*}

Q) How to select \(\mu_t\) in state \(i_t\) \ldots{} \(\mu_{t+1}\) in state \(i_{t+1}\)

\textbf{Possibility 1 (SARSA)} (State Action Reward State Action) (on-policy)
\begin{align*}
\mu_t &=
  \begin{cases}
    \arg\min_{\mu} Q_t(i_t, \mu) \text{ with p } 1-\epsilon \\
    \text{random action} \text{ with p } 1-\epsilon \\
  \end{cases} \\
\mu_{t+1} &=
  \begin{cases}
    \arg\min_{\mu} Q_t(i_{t_+1}, \mu) \text{ with p } 1-\epsilon \\
    \text{random action} \text{ with p } 1-\epsilon \\
  \end{cases}
\end{align*}

\textbf{Possibility 2 (Q-learning)} (off-policy)
\begin{align*}
\mu_t &=
  \begin{cases}
    \arg\min_{\mu} Q_t(i_t, \mu) \text{ with p } 1-\epsilon \\
    \text{random action} \text{ with p } 1-\epsilon \\
  \end{cases} \\
\mu_{t+1} &= \arg\min_{\mu} Q_t(i_{t+1}, \mu)
\end{align*}
target: greedy
behaviour: epsilon greedy

\section{On-policy vs off-policy methods (02/03/2023) (Chapter 5 of Sutton-Barto)}
\label{sec:org3b57990}

On-policy: data available from the policy for which we wish to find the value function
Off-policy: data from a given policy is to be used to find value function of another policy (policy is hardwired)

\textbf{Eg:} Traffic signal control

Phase : A set of signals that go green together
Q) Can we dynammically allocate green time to the phases?
cost = sum of queue lengths at all junctions

\subsection{Problem:}
\label{sec:orgdb870fe}
\begin{itemize}
\item Data is available from a behaviour policy (b)
\item We want to estimate value function of another policy (\(v_{\pi}(s)\)) -> target policy (\(\pi\))
\end{itemize}

Importance Sampling:
Consider
\begin{align*}
P(A_t, S_{t+1}, A_{t+1}, ..., S_T | S_t, A_{t=T-1} \sim \pi) &=
 P(S_T | S_{T-1}, A_{T-1}, ..., S_{t+1}, A_t, S_t, A_{t=T-1} \sim \pi) \\
&\times P(S_{T-1}, A_{T-1}, ..., S_{t+1}, A_t | S_t, A_{t=T-1} \sim \pi) \\
&= P(S_T | S_{T-1}, A_{T-1}) \pi(A_{T-1} | S_{T-1}) P(S_{T-1} | S_{T-2}, A_{T-2}) \pi(A_{T-2} | S_{T-2}) \\
&= \Pi_{k=t}^{T-1} \pi(A_k | S_k) p(S_{k+1} | S_k, A_k)
\end{align*}
Similarly,
\begin{align*}
P(A_t, S_{t+1}, A_{t+1}, ..., S_T | S_t, A_{t=T-1} \sim b) &= \Pi_{k=t}^{T-1} b(A_k | S_k) p(S_{k+1} | S_k, A_k)
\end{align*}
Define the importance sampling ratio as
\begin{align*}
P_{t=T-1} &= \frac{P(A_t, S_{t+1}, A_{t+1}, S_{t+2}, ..., S_T | S_t, A_{t=T-1} \sim \pi)}{P(A_t, S_{t+1}, A_{t+1}, S_{t+2}, ..., S_T | S_t, A_{t=T-1} \sim b)} \\
          &= \frac{\Pi_{k=t}^{T-1} \pi(A_k | S_k) \cancel{p(S_{k+1} | S_k, A_k)}}{\Pi_{k=t}^{T-1} b(A_k | S_k) \cancel{p(S_{k+1} | S_k, A_k)}}
          = \Pi_{k=t}^{T-1}\frac{\pi(A_k | S_k)}{b(A_k | S_k)}
\end{align*}

Note, we may estimate \(v_b(s) = \mathbb{E}[G_t | S_t = s, b]\), \(G_t = g(S_t, S_{t+1}) + \gamma g(S_{t+1}, S_{t+2}) + ... + \gamma^{T-t-1}g(S_{T-1}, S_T)\)
Consider
\begin{align*}
\mathbb{E}[P_{t=T-1} G_t | S_t = s, b] &= \mathbb{E}\left[\left( \Pi_{k=t}^{T-1} \frac{\pi(A_k|S_k)}{b(A_k|S_k)} \right) G_t \middle| S_t = s, b \right] \\
\end{align*}
This expectation is w.r.t. dist \(P(A_t, S_{t+1}, ..., S_T | S_t, A_{t=T-1} \sim b)\).
Thus \(\mathbb{E}[P_{t=T-1} G_t | S_t = s, b] = v_{\pi}(s)\)

\subsection{Monte-Carlo algorithm (estimates \(v_{\pi}(s)\) from data coming according to b)}
\label{sec:org8208a52}
Let \$\(\tau\)(s) = \$ set of all time steps in which state s is visited. (every visit method)
    \(T(t) =\) first time after t that termination happens

\(\{G_t\}_{t \in \tau(s)}\) are the returns pertaining to state S and \(\{P_{t=T(t) - 1}\}_{t \in \tau(s)}\) are the corresponding IS ratios.

\subsection{Regular Monte-Carlo estimate:}
\label{sec:org4621df8}
\begin{align*}
v(s) = \frac{\sum_{t \in \tau(s)} p_{t=T(t)-1}G_t}{|\tau(s)|} \\
\end{align*}
\subsection{Low variance estimate}
\label{sec:orgebea683}
\subsection{Incremental Implementation}
\label{sec:orga9200f5}

Let \(W_i = p_{t_i: T(t_i)-1}\), where \(t_i =\) ith time that state i is visited on the concateneted trajectory
\begin{align*}
V_{n+1} &= \frac{\sum_{k=1}^{n+1} W_k G_k}{\sum_{k=1}^{n+1} W_k}
         = \frac{\sum_{k=1}^n W_k G_k + W_{n+1}G_{n+1}}{\sum_{k=1}^{n+1} W_k} \\
        &= \left( \frac{\sum_{k=1}^n W_k}{\sum_{k=1}^{n+1} W_k} \right) \frac{\sum_{k=1}^n W_k G_k}{\sum_{k=1}^n W_k} +
            \frac{W_{n+1} G_{n+1}}{\sum_{k=1}^{n+1} W_k} \\
        &= \left( \frac{\sum_{k=1}^n W_k}{\sum_{k=1}^{n+1} W_k} \right) V_n + \frac{W_{n+1} G_{n+1}}{\sum_{k=1}^{n+1} W_k} \\
        &= V_n + \frac{W_{n+1}}{\sum_{k=1}^{n+1} W_k} \left( G_{n+1} - V_n \right)
\end{align*}
Let \(C_n = \sum_{k=1}^n W_k\) (Cumulative sum of weights for 1st n returns) and \(C_0 = 0\)
Then \(C_{n+1} = C_n + W_{n+1}\) and \(V_{n+1} = V_n + \frac{W_{n+1}}{C_{n+1}} [G_{n+1} - V_n]\).
The above formula will also sork for on-policy by letting \(W_n = 1, \forall n\)

\subsection{Important (for off-policy methods)}
\label{sec:org8ecc582}
Assumption of coverage:

If \(\pi(a|s) > 0\) for any \(a \in A(s)\)
then \(b(a|s) > 0\) for that \(a \in A(s)\)
\(\implies\) support of b should contain the support of \(\pi\)

\section{(09/03/2023)}
\label{sec:orgd14711d}
We need to show that
\begin{align*}
\mid \min_{v \in A(j)} Q(j, v) - \min_{v \in A(j)} \overline{Q}(j, v) \mid
\leq \max_{v \in A(j)} \mid Q(j, v) - \overline{Q}(j, v) \mid \\
\end{align*}

Note: If \(A \subset B\), then
\begin{align*}
  \inf_{x \in A} f(x) \geq \inf_{x \in B} f(x)
\end{align*}

infimum -> greatest lower bound
supremum -> least upper bound

Thus,
\begin{align*}
\inf_{x\in A} (f(x) + g(x)) = \inf_{x \in A, y = x} (f(x) + g(y)) \geq \inf_{x,y \in A} (f(x) + g(y))
\end{align*}

\begin{align*}
 \implies &\inf_{x \in A} ( (f-g)(x) + g(x)) \geq \inf_{x \in A} g(x) \\
 \implies &\inf_{x \in A} (f-g)(x) \leq \inf_{x \in A} f(x) - \inf_{x \in A} g(x) \\
          &\text{Let $h(x) = -g(x)\, \forall x$} \\
 \text{Then } &\sup_{x \in A} h(x) = \sup_{x \in A} (-g(x)) = - \inf_{x \in A} g(x) \\
 \implies &\inf_{x \in A} (f(x) + h(x)) \leq \inf_{x \in A} f(x) + \sup_{x \in A} h(x) \\
 \implies &\inf_{x \in A} (f(x) + h(x)) - \inf_{x \in A} f(x)  \leq \sup_{x \in A} h(x) \\
          &\text{Let $h(x) = g(x) - f(x)$} \\
 \implies &\inf_{x \in A} g(x) - \inf_{x \in A} f(x) \leq \sup_{x \in A} (g(x) - f(x)) \\
 \implies &\inf_{x \in A} g(x) - \inf_{x \in A} f(x) \leq \sup_{x \in A} \mid g(x) - f(x) \mid
 \end{align*}

\textbf{Claim:}

\begin{align*}
 \mid \sup_{x \in A} (g(x) - f(x)) \mid \leq \sup_{x \in A} \mid g(x) - f(x) \mid
\end{align*}

Case (i):
\begin{align*}
&\sup_{x \in A} (g(x) - f(x)) \geq 0 \\
\implies &\sup_{x \in A} (g(x) - f(x)) \leq \sup_{x \in A} \mid g(x) - f(x) \mid \\
\end{align*}

Case (ii):
\begin{align*}
&\sup_{x \in A} (g(x) - f(x)) < 0 \\
&\mid g(x) - f(x) \mid = - (g(x) - f(x)) \, \forall x
\end{align*}

\begin{align*}
\implies &\mid \sup_{x \in A} (g(x) - f(x)) \mid = - \sup_{x \in A} (g(x) - f(x))
= \inf_{x \in A} (-(g(x) - f(x)))
= \inf_{x \in A} \mid g(x) - f(x) \mid
\sup_{x \in A} \mid g(x) - f(x) \mid \\
\implies &\inf_{x \in A} g(x) - \inf_{x \in A} f(x) \leq \sup_{x \in A} \mid g(x) - f(x) \mid
\end{align*}
Also,
\begin{align*}
\implies &\inf_{x \in A} f(x) - \inf_{x \in A} g(x) \leq \sup_{x \in A} \mid g(x) - f(x) \mid \\
\implies &\mid \inf_{x \in A} f(x) - \inf_{x \in A} g(x) \mid \leq \sup_{x \in A} \mid g(x) - f(x) \mid
\end{align*}

Thus it follows that
\begin{align*}
\mid \min_{v \in A(j)} Q(j, v) - \mion_{v \in A(j)} \overline{Q}(j, v) \mid \leq \max_{v \in A(j)} \mid Q(j, v) - \overline{Q} (j, v) \mid
\end{align*}

\section{Function Approximations based approaches for Reinforcement Learning (09/03/2023)}
\label{sec:orgb25e3a0}

Suppose each route has a buffer that can store 1000 packets.
Q-learning and Sarsa algorithms, based on lookup table updates cannot be applied.

We need to resort to approximations
\begin{itemize}
\item Value function approximations (Temporal difference learning, Q-Learning, \ldots{})
\item Policy approximations (policy gradient methods, actor critic methods, \ldots{})
\end{itemize}

\subsection{Value function approximations (09/03/2023)}
\label{sec:orgd37a5ed}

Given policy \(\pi\), value function
\begin{align*}
v_{\pi}(s) = \lim_{N \rightarrow \infty} \mathbb{E} \left[ \sum_{k=0}^{N-1} \gamma^k g(i_k, \pi(i_k), i_{k+1}) \middle| i_0 = s \right] \forall s \in S
\end{align*}
Let \(v_{\pi}(s) \approx \hat{v}(s, w)\) where \(w \in \mathbb{R}^d\) is a parameter
Invariably, \(d << |s|\)

Examples:
\subsubsection{(i) Linear approximation architectures}
\label{sec:org3685ce7}
\begin{align*}
\hat{v}(s, w) = w^T \phi(s)
\end{align*}
Where \(\phi(s) = (\phi_1(s), \phi_2(s), ..., \phi_d(s))^T\)(feature of state s, can be highly non linear), \(w = (w_1, w_2, ..., w_d)^T\)

Examples of LFA:
\begin{enumerate}
\item (a) polynomial features
\label{sec:org3602b7c}
suppose \(s = (s_1, s_2)^T\)

Polynomial representations:
\begin{align*}
\rightarrow &\phi(s) = (1, s_1, s_2, s_1s_2)^T \\
\rightarrow &\phi(s) = (1, s_1, s_2, s_1s_2, s_1^2s_2, s_1s_2^2, s_1^2s_2^2)^T
\end{align*}
\item (b) Fourier bases
\label{sec:org272eb4f}
Example:
Let \(s = (s_1, s_2, ..., s_k)^T\) with each \(s_i \in [0, 1]\),
Then \(\phi_i(s) = cos(\pi s^T c^i)\), where \(c^i = (c_1^i, ..., c_k^i)^T\) with \(c_j^i \in \{0, 1, ..., n\}, j = 1, ..., k\)
\(c^i\) takes \((n+1)^k\) values
\(s^T c^i\) has the effect of assuming an integer in \(\{0, 1, ...n\}\) to each \_ of s
The integer determines the feature frequency along that dim
\end{enumerate}
\subsubsection{(ii) Nonlinear approximaiton architectures (neural nets based architectures)}
\label{sec:org1b67b2c}

\begin{align*}
\hat{v} (s, w) = w^T \phi(s)
\end{align*}
Prediction Error objective:
\begin{align*}
\overline{VE}(w) = \sum_{s \in S} \mu(s) (v_{\pi}(s) - \hat{v}(s, w))^2
\end{align*}
Here, \(\mu(s), s \in S\) is the steady state distribution of the markov chain unser the given policy
Let \(\mu(s) > 0 \,\forall s \in S\)

\begin{align*}
\{x_t\} \text{ or } \{S_t\} \rightarrow p^{\pi}(s, s') = \sum_{a \in A(s)} \pi(a | s)p(s' | s < a)
\end{align*}

Goal: Find \(w^{*}\) that minimizes \(\overline{VE}(w)\) which implies that distribution of \(\hat{v}(s, w^{*})\) from \(v_{\pi}(s)\) is the minimum over all \(\hat{v}(s, w)\)

Lets use gradient search
\begin{align*}
w_{t+1} = w_t - \frac{1}{2} \alpha \nabla \overline{VE}(w_t)
\end{align*}

\begin{align*}
\nabla \overline{VE}(w_t) &= \nabla_w \left( \sum_{s \in S} \mu(s)(v_{\pi}(s) - \hat{v}(s, w))^2 \right) \\
                          &= \sum_{s \in S} \mu(s)\nabla_w (v_{\pi}(s) - \hat{v}(s, w))^2 \\
                          &= -2 \sum_{s \in S} \mu(s)(v_{\pi}(s) - \hat{v}(s, w)) \nabla_w \hat{v}(s, w)
\end{align*}
The algorithm then is
\begin{align*}
w_{t+1} = w_t + \alpha \sum_{s \in S} \mu(s)(v_{\pi}(s) - \hat{v}(s, w)) \nabla_w \hat{v}(s, w)
\end{align*}
Problems with this update rule:
(i) we don't know \(\mu(s)\)
(ii) we don't know \(v_{\pi}(s)\)

Use stochastic approximation (i.e. we use SGD(stochastic gradient descent))
\begin{align*}
w_{t+1} = w_t + \alpha (v_{\pi}(s_t) - \hat{v}(s_t, w_t)) \nabla_w \hat{v}(s_t, w_t), \text{ $s_t$ is the state visited at time t}
\end{align*}

Also, \(\mathbb{E}_0 [v_{\pi}(s_t)] = \sum_{s \in S}\mu(s) v_{\pi}(s)\) where \(\mathbb{E}_0\) is the expectation under the stationary list of the Markov chain \(\{S_t\}\)

2nd Problem: Instead of \(v_{\pi}(s_t)\) use \(G_t\) (gradient Monte-Carlo)


\subsection{Prediction Error Objective (14/03/2023)}
\label{sec:org85af530}
\begin{align*}
\overline{VE}(w) = \sum_s \mu(s) \left( v_{\pi}(s) - \hat{v}(s, w) \right)^2
\end{align*}
\(\mu(s)\) : average time spent in state \(s\) by the Markov chain \(\{S_t\}\).
\(\hat{v}(s, w)\) : approximate value function is a parameterized space with parameter \(w \in \mathbb{R}^l\)

\textbf{Relaxed objective}: Find a local minimum instead
\textbf{Update rule}: Gradient Search
\begin{align}
w_{t+1} &= w_t - \frac{1}{2}\alpha \nabla \overline{VE}(w_t) \\
        &= w_t + \alpha \sum_{s \in S}\mu(s) \left( v_{\pi}(s) - \hat{v}(s, w_t) \right) \nabla \hat{v} (s, w_t)
\end{align}

\(\mu(s)\) is not known

\textbf{Sample based update}
\begin{align*}
w_{t+1} &= w_t + \alpha (v_{\pi}(s_t) - \hat{v}(s_t, w_t)) \nabla \hat{v}(s_t, w_t)
\end{align*}
\(s_t\) : state visited at time \(t\)

Will work because

Steady state expectation:
\begin{align*}
E_0[v_{\pi}(s_t)] &= \sum_{s \in S} \mu(s) v_{\pi}(s)
\end{align*}
problem is that we don't know \(v_{\pi}(s_t)\)

\subsubsection{Gradient Monte-Carlo Algorithm}
\label{sec:orgb23dd70}

\begin{align*}
W_{t+1} &= w_t + \alpha \left( G_t - \hat{v}(s_t, w_t) \right) \nabla\hat{v}(s_t, w_t) \\
    G_t &= (\gamma(s_t, \pi(s_t), s_{t+1}) + Y\gamma(s_{t+1}, \pi(s_{t+1}), s_{t+2}) + ... + Y^{T-t-1} \gamma(s_{T-1}, \pi(s_{T-1}), s_T)) \\
\end{align*}

\(G_t\) : return on the episode starting from state \(s_t\) (could be first visit return or that obtained using every visit procedure)

\subsubsection{Alternative to trajectory-based methods (incremental update methods)}
\label{sec:orgbf30100}
TE(0) with function approximation

Recall the Bellman Equation for a given policy \(\pi\)
\begin{align*}
v_{\pi}(s) =&= E_{s'}\left[ \gamma(s, \pi(s), s') + Yv_{\pi}(s') \right]
\end{align*}

Recall that in TD(0) without function approximation,
Then estimate \(v(s)\) of \(v_{\pi}(s)\) is \(\gamma (s, \pi(s), s') + Yv(s')\)
\begin{align*}
v_{\pi}(s) &= E \left[ G_t \middle S_t = s \right] \\
           &= E \left[ \sum_{t=1}^{\infty} Y^t \gamma(s_t, \pi(s_t), s_{t+1}) \middle S_0 = s \right]
\end{align*}

\begin{enumerate}
\item TD(0) algorithm with function approximation
\label{sec:org9fe2718}
\begin{align*}
w_{t+1} &= w_t + \alpha (\gamma(s_t, \pi(s_t), s_{t+1}) + Y\hat{v}(s_{t+1}, w_t) - \hat{v}(s_t, w_t)) \nabla \hat{v}(s_t, w_t)
\end{align*}

\uline{Important Special case} (TD(0) with LFA):

Linear function approximation:  \(\hat{v}(s, w) &= w^T\phi(s)\)
\(\phi(s)\) : state features, \(w \in \mathbb{R}^d\), \(\phi(s) \in \mathbb{R}^d\)

Under LFA,
\(\nabla \hat{v}(s_t, w_t) = \phi(s_t)\)
\begin{align*}
w_{t+1} &= w_t + \alpha(\gamma(s_t, \pi(s_t), s_{t+1}) + Yw_t^T\phi(s_{t+1}) - w_t^T\phi(s_t)) \phi(s_t) \\
        &= w_t + \alpha(\gamma(s_t, \pi(s_t), s_{t+1}) +w_t^T(Y\phi(s_{t+1}) - \phi(s_t))) \phi(s_t) \\
        &= w_t + \alpha\phi(s_t) (\gamma(s_{t}, \pi(s_t), s_{t+1}) + (Y\phi(s_{t+1}) - \phi(s_t))^Tw_t)
\end{align*}


Consider the LFA architecture, \(\hat{v}(i, w) = \phi(i)^Tw\)
Here, \(w = (w_1, ..., w_d)^T\), \(\phi(i) = (\phi_1(i), \phi_2(i), ..., \phi_d(i))^T\)

Let the feature matrix \(\Phi = \begin{bmatrix} \phi(1)^T \\ \phi(2)^T \\ \vdots \\ \phi(|s|)^T \\ \end{bmatrix}_{|s| \times d}\)

Let \(\hat{v}_w = (\hat{v}(i, w), i\in S)^T\), then

\begin{align*}
\hat{v}_w = \Phi w = \begin{bmatrix} \phi_1(1) \\ \phi_1(2) \\ \vdots \\ \phi_1(|s|) \\ \end{bmatrix} w_1
                   + \begin{bmatrix} \phi_2(1) \\ \phi_2(2) \\ \vdots \\ \phi_2(|s|) \\ \end{bmatrix} w_2
                   + ...
                   + \begin{bmatrix} \phi_d(1) \\ \phi_d(2) \\ \vdots \\ \phi_d(|s|) \\ \end{bmatrix} w_d
\end{align*}

Let \(\phi_i = \begin{bmatrix} \phi_i(1) \\ \phi_i(2) \\ \vdots \\ \phi_i(|s|) \end{bmatrix}\) : ith feature vector or ith basis vector

Let \(S_0 = \{ \Phi w | w \in \mathbb{R}^d \}\) denote hte space of linear function approximations parameterized by \(w \in \mathbb{R}^d}\)

\item Assumptions:
\label{sec:orgdb548bd}
\begin{enumerate}
\item The Markov Chain \(\{ S_n \}\) has steady-state probabilities \(\zeta_1, \zeta_2, ..., \zeta_{|s|}\) with \(\zeta_j > 0\) \(\forall j \in S\)
\item The matrix \(\Phi\) has rank d and \(|s| \geq d\)
\end{enumerate}

\item Projected Bellman Equation
\label{sec:org1347e5f}
Define a weighted Euclidean norm on \(\mathbb{R}^{|s|}\) as
\begin{align*}
\lVert V \rVert_x = \sqrt{V^T \times V} = \sqrt{\sum_{i=1}^{|s|}x_i(v(i))^2}
\end{align*}
Here, \(X = \begin{bmatrix} x_1 & \dots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \dots & x_{|s|}\end{bmatrix}\)
Assume \(x_1, x_2, ..., x_{|s|} > 0\)
Let \(\pi\) be the projection operator from \(\mathbb{R}^{|s|}\) to \(s_0\) w.r.t. \(\lVert . \rVert_x\).
Thus for any \(v \in \mathbb{R}^{|s|}\), TTV is the unique vector is \(s_0\) that minimizes \(\lVert v - \hat{v} \rVert_x^2\) over all \(\hat{v} \in S_0\).
Since \(\Phi\) has rank \(d\), any \(\hat{v} \in S_0\) is uniquely written as \(\hat{v} = \Phi w\) for some \(w \in \mathbb{R}^d\)

\begin{align*}
\implies \lVert v - \hat{v} \rVert_x^2 &= \lVert v - \Phi w \rVert_x^2 = (v - \Phi w)^T x (v - \Phi w) \\
\end{align*}
Thus \(\pi v &= \Phi w_v\), where \(w_v = \arg\min_{w \in \mathbb{R}^d} \lVert v - \Phi w \rVert_x^2\), \(v \in \mathbb{R}^{|s|}\)

In order to find \(w_v\), compute \(\nabla_w(\lVert v - \Phi w \rVert_x^2)\) and set it to 0, then
\begin{align*}
\nabla_w (\lVert v - \Phi w \rVert_x^2 &= \nabla_w(\lVert v - \Phi w \rVert_x^2) \\
&= \nabla_w((v - \Phi w)^T \times (v - \Phi w)) \\
&= \nabla_w (v^T \times v - w^T\Phi^Tv - v^T \times \Phi w + w^T \Phi^T \times \Phi w) \\
&= -2\Phi^T \times v + 2 \Phi^T \times \Phi w = 0 \\
\implies \Phi^T \times v &= (\Phi^T \times \Phi) w_v \\
\implies w_v &= (\Phi^T \times \Phi)^{-1} \Phi^T \itmes v.
\end{align*}

Thus, the point \(\hat{v}\) in \(s_0\) correspoiding to parameter \(w_v\) is \(\hat{v} = \Phi w_v = \Phi(\Phi^T \times \Phi)^{-1} \Phi^T v = \pi v\)
Note: \((\Phi^T \times \Phi)\) : positive definite matrix, since \(\Phi\) has rank d and \(x\) has all positive values.
\end{enumerate}

\subsection{Projected Bellman Equation (16/03/2023)}
\label{sec:orga71d6e7}

Define a weighted Euclidean norm on \(\mathbb{R}^{|s|}\) as
\begin{align*}
\lVert J \rVert_{\xi} = \sqrt{J^TDJ} = \sqrt{\sum_{i=1}^{|s|} \xi_i J(i)^2}
\end{align*}
\(\xi = (\xi_1, ..., \xi_{|s|})^T\) is the stationary distribution of \(\{S_t\}\)
\(D = \begin{bmatrix} \xi_1 & & 0 \\ & \ddots & \\ 0 & & \xi_{|s|} \end{bmatrix}\)

Let \(\pi\) be the projection operator onto \(S_0 = \{ \Phi w | w \in \mathbb{R}^d\}\)
for any \(J \in \mathbb{R}^{|s|}\), \(\Pi J\) is the unique vector in \(S_0\) that minimizes \(\lVert J - \hat{J} \rVert_{\xi}\) over all \(\hat{J} \in S_0\)

Since \(\Phi\) has rank \(d\), any \(\hat{J} \in S_0\) is uniquely written as \(\hat{J} = \Phi w\) for some \(w \in \mathbb{R}^d\)

\begin{align*}
\lVert J - \hat{J} \rVert_{\xi}^2 &= \lVert J - \Phi w \rVert_{\xi}^2 = (J - \Phi w)^TD(J - \Phi w)
\end{align*}


\(\therefore \Pi J = \Phi w_J\) where \(w_J = \arg\min_{w \in \mathbb{R}^{|s|}} \lVert J - \Phi w \rVert_{\xi}^2\), \(J \in \mathbb{R}^{|s|}\)

In order to find \(w_J\),
\begin{align*}
&\nabla_w (\lVert J - \Phi w \rVert_{\xi}^2) = 0 \\
&\Phi^TD(J - \Phi w_J) = 0
\end{align*}

For any \(w \in \mathbb{R}^d\), \(\Phi w \in S_0\) \(\implies w^T\Phi^T D (J - \Phi w_J) = 0\)
\begin{align*}
\implies w_J &= (\Phi^TD\Phi)^{-1} \Phi^TDJ \\
\implies \Phi w_J &= \Phi(\Phi^TD\Phi)^{-1} \Phi^TD J \\
\implies \Pi &= \Phi(\Phi^TD\Phi)^{-1}\Phi^TDJ
\end{align*} 

Any vectors \(x, y\) are orthogonal if \(x^TDy = 0 \implies \sum_{i=1}^{|s|} \xi_ix_iy_i = 0\)

Recall Bellman Equation for policy \(\pi\),

\begin{align*}
J &= T_{\pi}J \\
\implies T_{\pi}J &= ((T_{\pi}, J)(i), i \in S)^T
\end{align*}
where \((T_{\pi}J)(i) = \sum_{j \in S} p_{ij}(\pi(i))(g(i, \pi(i), j) + \gamma J(j))\)

\uline{Projected Bellman Equation}: \(\Phi w = \Pi T_{\pi}(\Phi w)\)
View \(\Pi T_{\pi}\) as a compositoin of \(\Pi\) and \(T_{\pi}\)

\subsubsection{Lemma 1:}
\label{sec:org6a69806}
\(\lVert P_{\pi} z \rVert_{\xi} \leq \lVert z \rVert_{\xi}\) \(\forall z \in \mathbb{R}^{|s|}\), \(P_{\pi} = \begin{bmatrix} P_{\pi}(1,1) & \dots & P_{\pi}(1, |s|) \\ \vdots & \ddots & \vdots \\ P_{\pi}(|s|, 1) & \dots & P_{\pi}(|s|, |s|) \end{bmatrix}\)


\begin{align*}
\lVert P_{\pi}z \rVert_{\xi}^2 &= \sum_{i=1}^{|s|} \xi_i \left( \sum_{j=1}^{|s|}p_{ij}z_j \right)^2
\leq \sum_{i=1}^{|s|}\xi_i \sum_{j=1}^{|s|}p_{ij}z_j^2 \\
&= \sum_{j=1}^{|s|} \left( \sum_{i=1}^{|s|} \xi_i p_{ij} \right) z_j^2
= \sum_{j=1}^n \xi_jz_j^2 = \lVert z \rVert_2^2
\end{align*}

\(\xi = (\xi_1, \xi_2, ..., \xi_{|s|})^T\) is the stationary distribution of \(\{S_n\}\) under policy \(\pi\)
\(\xi^TP_{\pi} = \xi^T\) as \(\xi(i)^{T}P_{\pi} = \xi(i+1)\)

Thus \(\lVert P_{\pi} z \rVert_{\xi} \leq \lVert z \rVert_{\xi}\)

\subsubsection{Lemma 2:}
\label{sec:orge6cea86}
The projection map \(\Pi\) is non-expansive, i.e., \(\lVert \Pi J - \Pi \overline{J} \rVert_{\xi} \leq \lVert J - \overline{J} \rVert_{\xi}\) \(\forall J, \overline{J} \in \mathbb{R}^{|s|}\)
Note that,
\begin{align*}
\lVert \Pi (J - \overline{J}) \rVert_{\xi}^2 &\leq \lVert \Pi (J - \overline{J}) \rVert_{\xi}^2 + \lVert (I - \Pi)(J - \overline{J}) \rVert_{\xi}^2\\
&= \lVert \Pi (J - \overline{J}) \rVert_{\xi}^2 + \lVert (J - \overline{J}) - \Pi(J - \overline{J})\rVert_{\xi}^2
\end{align*}
Note: \(\Pi (J - \overline{J}) \perp ((J - \overline{J}) - \Pi(J - \overline{J}))\)
Therefore by Pythagorean theorem,
\begin{align*}
\lVert \Pi (J - \overline{J}) \rVert_{\xi}^2 &\leq \lVert \Pi (J - \overline{J}) \rVert_{\xi}^2 + \lVert (I - \Pi)(J - \overline{J}) \rVert_{\xi}^2\\
&= \lVert \Pi(J - \overline{J}) + (I - \Pi)(J - \overline{J}) \rVert_{\xi}^2 = \lVert J - \overline{J} \rVert_{\xi}^2 \\
\implies \lVert \Pi (J - \overline{J}) \rVert_{\xi}^2 &\leq \lVert J - \overline{J} \rVert_{\xi}^2 \\
\implies \lVert \Pi (J - \overline{J}) \rVert_{\xi} &\leq \lVert J - \overline{J} \rVert_{\xi}
\end{align*}

\uline{Proposition}: Let \(\Pi r^{*}\) be the fixed point of \(\Pi T_{\pi}\). Then
\begin{align*}
\lVert J_{\pi} - \Phi r^{*} \rVert_{\xi} \leq \frac{1}{\sqrt{1 - \gamma^2}}\lVert J_{\pi} - \Pi J_{\pi} \rVert_{\xi}
\end{align*}
Note that:
\begin{align*}
\lVert J_{\pi} - \Phi r^{*} \rVert_{\xi}^2 &= \lVert J_{\pi} - \Pi J_{\pi} \rVert_{\xi}^2 + \lVert \Pi J_{\pi} - \Phi r^{*} \rVert_{\xi}^2 \\
&= \lVert J_{\pi} - \PiJ_{\pi}\rVert_{\xi}^2 + \lVert \Pi T_{\pi}J_{\pi} - \Pi T_{\pi}(\Phi r^{*}) \rVert_{\xi}^2
\end{align*}
(Since \(J_{\pi} = T_{\pi}J_{\pi}\) and \(\Phi r^{*} = \Pi(T_{\pi}(\Phi r^{*})\))

Note that:
\begin{align*}
\lVert \Pi T_{\pi}J_{\pi} - \Pi T_{\pi}(\Phi r^{*}) \rVert_{\xi} &\leq \lVert T_{\pi}J_{\pi} - T_{\pi}(\Phi r^{*}) \rVert_{\xi} \text{  (by non-expansivity of $\Pi$)} \\
&\leq \gamma \lVert J_{\pi} - \Phi r^{*} \rVert_{\xi} \text{  (by contraction property of $T_{\pi}$)} \\
\end{align*}

Chapter 6, Vol 2 of Bertsekas (Approximate DP)

\begin{align*}
\lVert J_{\pi} - \Phi r^{*} \rVert_{\xi}^2 &\leq \lVert J_{\pi} - \Pi J_{\pi} \rVert_{\xi}^2 + \gamma^2\lVert J_{\pi} - \Phi r^{*} \rVert_{\xi}^2 \\
\implies (1 - \gamma^2)\lVert J_{\pi} - \Phi r^{*} \rVert_{\xi}^2 &\leq \lVert J_{\pi} - \Pi J_{\pi} \rVert_{\xi}^2 \\
\implies \lVert J_{\pi} - \Phi r^{*} \rVert_{\xi} &\leq \frac{1}{\sqrt{(1 - \gamma^2)}}\lVert J_{\pi} - \Pi J_{\pi} \rVert_{\xi} \\
\end{align*}
This is the error

\(r^{*} = \arg\min_{w \in \mathbb{R}^d} \lVert \Phi w - (g + \gamma P_{\pi} \Phi r^{*}) \rVert_{\xi}^2\)

\(\Phi^TD(I - \gamma P_{\pi})\Phi r^{*} = \Phi^TDg \implies C r^{*} = d \implies r^{*} = C^{-1}d\), where \(C_{d \times d} = \Phi^TD(I - \gamma P_{\pi})\Phi\), \(d = \Phi^TDg\)

True Bellman Solution: \(J_{\pi} = (I - \gamma P_{\pi})^{-1}_{|s| \times |s|}g\)

Numerical Solution to the projected Bellman Equation:
Projected value iteratoin (PVI):
\begin{align*}
\Phi r_{k+1} = \Pi T_{\pi} \Phi r_k, k=0,1,2,...
\end{align*}
Select \(r_0 \in \mathbf{R}^d\) arbitrarily
We know that \(\Pi T_{\pi}\) is a contraction
\begin{align*}
r_{k+1} = \arg\min_{w \in \mathbb{r}^d} \lVert \Phi w - (g + \gamma P_{\pi} \Phi r_k \rVert^2_{\xi}
\end{align*}

Consider again
\begin{align*}
&\nabla_w (\Phi w - g - \gamma P_{\pi} \Phi r_k)^T D (\Phi w - g - \gamma P_{\pi} \Phi r_k)) = 0 \\
\implies &2 \Phi^TD(\Phi r_{k+1} - (g + \gamma P_{\pi} \Phi r_k)) = 0 \\
\implies &(\Phi^TD\Phi)r_{k+1} = \Phi^TDg + \gamma \Phi^TDP_{\pi} \Phi r_k \\
\implies &r_{k+1} = (\Phi^TD\Phi)^{-1}b + \gamma (\Phi^TD\Phi)_{-1}\Phi^TDP_{\pi}\Phi r_k
\implies &r_{k+1} = r_k + (\Phi^TD\Phi)^{-1}b + (\Phi^TD\Phi)^{-1}(\Phi^TD\Phi)r_k + \gamma (\Phi^TD\Phi)_{-1}\Phi^TDP_{\pi}\Phi r_k \\
\end{align*}




\section{Events}
\label{sec:org51a13d9}
\begin{itemize}
\item[{$\boxtimes$}] Quiz 1: Jan 19
\item[{$\boxtimes$}] Midterm 1: Feb 16
\item[{$\square$}] Midterm 2 and Quiz 2: Mar 30
\item[{$\boxtimes$}] Assignment 1: Feb 04
\item[{$\square$}] Assignment 1: Mar 19
\item[{$\square$}] Project
\item[{$\square$}] Endterm
\end{itemize}
\end{document}