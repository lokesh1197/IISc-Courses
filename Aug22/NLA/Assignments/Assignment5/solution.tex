\documentclass[12pt, letterpaper]{article}
\usepackage[margin=0.5in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathabx}
% \usepackage{minted}
\usepackage{graphicx}
\usepackage{pdfpages}
% \graphicspath{ {./images/} }

\usepackage{fancybox}
\setlength\parindent{0pt}

\title{Assignment 5}
\author{Lokesh Mohanty (SR no: 21014)}
\date{November 2022}

\begin{document}
\fontsize{14pt}{18pt}\selectfont

\maketitle

\includepdf[pages=-]{Problem 1.pdf}

\section*{Problem 2}

\paragraph{(a)} False

\textbf{Pure QR:}
\begin{align*}
  &\mathbf{A}^{(0)} = \mathbf{A} \\
  &\mathbf{A}^{(1)} = \mathbf{R}^{(1)}\mathbf{Q}^{(1)}
  = (\mathbf{Q}^{(1)})^T\mathbf{A}\mathbf{Q}^{(1)} \\
\end{align*}

\textbf{Simultaneous Iteration:}
\begin{align*}
  &\mathbf{A}^{(1)} = \mathbf{R}^{(1)}\mathbf{Q}^{(1)}
  = (\mathbf{Q}^{(1)})^T\mathbf{A}\mathbf{Q}^{(0)}\mathbf{Q}^{(1)} \\
\end{align*}

Hence Pure QR algorithm is equivalent to Simultaneous Iteration only when $\mathbf{Q}^{(0)} = \mathbf{I}$.

\paragraph{(b)} False
\begin{align*}
  \mathbf{A}^{(k)} &= (\mathbf{Q}^{(k)})^T\mathbf{A}^{(k-1)}\mathbf{Q}^{(k)} \\
  &= (\mathbf{\tilde{Q}}_k)^T \mathbf{A} \mathbf{\tilde{Q}}_k \\
  \mathbf{\tilde{Q}}_k &= \mathbf{Q}^{(1)}\mathbf{Q}^{(2)}...\mathbf{Q}^{(k)} \\
\end{align*}

Hence as $k \rightarrow \infty$, $\mathbf{\tilde{Q}}_k$ converges to the eigenvector matrix but not the sequence of matrices $\mathbf{Q}^{(k)}$

\paragraph{(c)} False. As at the kth iteration,
$\mathbf{A}^{(k)} = \mathbf{Q}^{(k+1)}\mathbf{R}^{(k+1)}$

\paragraph{(d)} True. As at each iteration,
\begin{itemize}
\item Pure QR: \\
  QR factorization: $\mathbf{A}^{(k-1)} = \mathbf{Q}^{(k)}\mathbf{R}^{(k)}$\\
  Matrix multiplication: $\mathbf{A}^{(k)} = \mathbf{R}^{(k)}\mathbf{Q}^{(k)}$\\
\item Simultaneous iteration: \\
  Matrix multiplication: $\mathbf{Z} = \mathbf{A}\mathbf{Q}^{(k-1)}$\\
  QR factorization: $\mathbf{Q}^{(k)}\mathbf{R}^{(k)} = \mathbf{Z}$\\
\end{itemize}

Since both the algorithms require FLOPS of the same order i.e., $O(m^3)$, they have the same computational complexity.

\pagebreak
\section*{Problem 3}

\paragraph{(a)} False. As we know that the roots of a polynomial equation of degree greater than or equal to 5 cannot be computed in finite number of steps and since a polynomial equation can always be assumed as the characteristic polynomial of a matrix(whose roots are the eigenvalues of the matrix), we can say that computing eigenvalues and eigenvectors of a matrix is not possible in a finite number of steps for $m \geq 5$.

\paragraph{(b)} False. Without reducing to tridiagonal form (phase I) first, each iteration of phase II will cost $O(m^3)$ flops with the total cost being $O(m^4)$. In case the symmetric matrices, there might be improvement in the number flops but it still is not of the order $O(m^3)$.

\paragraph{(c)} False. If $\mathbf{v}^{(0)}$ is equal to any eigenvector other than the one corresponding to the largest eigenvalue, then all the sequence of vectors will be the same eigenvector and hence will never converge to the largest eigenvalue.

\paragraph{(d)} True. Let $\lambda, \mathbf{v}$ be a eigenvalue-vector pair of $\mathbf{A}$, then

\begin{align*}
  \implies &\mathbf{A}\mathbf{v} = \lambda \mathbf{v} \\
  \implies &\mathbf{F}\mathbf{A}\mathbf{v} = \lambda \mathbf{F}\mathbf{v} \\
  \implies &(\mathbf{F}\mathbf{A} - \lambda \mathbf{F})\mathbf{v} = \mathbf{0} \\
           &\text{Let $\mathbf{u} = \mathbf{Fv}
             \implies \mathbf{v} = \mathbf{F}^{-1}\mathbf{u}$ and
             we know that $\mathbf{F}\mathbf{F}^{-1} = \mathbf{I}$} \\
  \implies &(\mathbf{F}\mathbf{A} - \lambda \mathbf{F})\mathbf{F}^{-1}\mathbf{u} = \mathbf{0} \\
  \implies &(\mathbf{F}\mathbf{A}\mathbf{F}^{-1} - \lambda \mathbf{F}\mathbf{F}^{-1})\mathbf{u} = \mathbf{0} \\
  \implies &(\mathbf{F}\mathbf{A}\mathbf{F}^{-1} - \lambda \mathbf{I})\mathbf{u} = \mathbf{0} \\
  \implies &\mathbf{F}\mathbf{A}\mathbf{F}^{-1}\mathbf{u} = \lambda \mathbf{u} \\
\end{align*}

Therefore $\mathbf{FAF}^{-1}$ and $\mathbf{A}$ have same eigenvalues.

\pagebreak
\section*{Problem 4}

\paragraph{(a)} Let $\lambda, \mathbf{u}$ be a eigenvalue-vector pair of $\mathbf{A}$, then
\begin{align*}
  \mathbf{A}\mathbf{u} &= \lambda \mathbf{u} \\
  \implies \mathbf{A}^T\mathbf{A}\mathbf{u} \\
             &= \lambda \mathbf{A}^T\mathbf{u} \\
             &= \lambda \mathbf{A}\mathbf{u} \\
             &= \lambda^2 \mathbf{u} \\
\end{align*}

Since eigenvalues of $\mathbf{A}^T\mathbf{A}$ are square of singular values($\sigma$) of $\mathbf{A}$,

\begin{align*}
  \implies &\lambda^2 = \sigma^2 \\
  \implies &|\lambda| = |\sigma| \\
  \implies &|\lambda| = \sigma \\
\end{align*}

\paragraph{(b)} We know that Rayleigh coefficient of any vector lies between the minimum and maximum eigenvalue and since the largest eigenvalue of a matrix is its second norm,

\begin{align*}
  &\left|\frac{\mathbf{x}^T\mathbf{Ax}}{\mathbf{x}^T\mathbf{x}}\right| \leq ||\mathbf{A}|| \\
  &\text{Since $\mathbf{x}$ is a unit vector,} \\
  \implies &|\mathbf{x}^T\mathbf{Ax}| \leq ||\mathbf{A}|| \\
\end{align*}

\paragraph{(c)}

\begin{align*}
  (\mathbf{A} + \delta \mathbf{A})(\mathbf{u} + \delta \mathbf{u})
  &= (\lambda + \delta \lambda)(\mathbf{u} + \delta\mathbf{u}) \\
  \implies \mathbf{A}\delta \mathbf{u} + \delta \mathbf{A}(\mathbf{u} + \delta \mathbf{u})
  &= \delta \lambda(\mathbf{u} + \delta \mathbf{u}) + \lambda \delta \mathbf{u} \\
  \implies \mathbf{u}^T\mathbf{A}\delta\mathbf{u}
  + \mathbf{u}^T\delta \mathbf{A}(\mathbf{u} + \delta \mathbf{u})
  &= \mathbf{u}^T\delta \lambda (\mathbf{u} + \delta \mathbf{u})
  + \lambda \mathbf{u}^T \delta \mathbf{u} \\
  \implies \mathbf{u}^T\delta \mathbf{A}(\mathbf{u} + \delta \mathbf{u})
  &= \mathbf{u}^T\delta\lambda(\mathbf{u} + \delta \mathbf{u}) \\
  \implies \delta\lambda &= \frac{\mathbf{u}^T\delta \mathbf{A}(\mathbf{u} + \delta \mathbf{u})}{\mathbf{u}^T(\mathbf{u} + \delta \mathbf{u})} \\
  &\text{we know that $\mathbf{u}^T(\mathbf{u} + \delta \mathbf{u}) \approx ||\mathbf{u}||||\mathbf{u} + \delta \mathbf{u}||$} \\
  &\text{and $\mathbf{u}^T\delta \mathbf{A}(\mathbf{u} + \delta \mathbf{u})
    \leq ||\mathbf{u}||||\delta \mathbf{A}(\mathbf{u} + \delta \mathbf{u})||$} \\
  \implies |\delta \lambda| &\leq \frac{||\mathbf{u}||||\delta \mathbf{A}(\mathbf{u} + \delta \mathbf{u})||}{||\mathbf{u}||||\mathbf{u} + \delta \mathbf{u}||} \\
  &\leq \frac{||\delta \mathbf{A}(\mathbf{u} + \delta \mathbf{u})||}{||(\mathbf{u} + \delta \mathbf{u})||} \\
  &\leq \frac{||\delta \mathbf{A}||||(\mathbf{u} + \delta \mathbf{u})||}{||(\mathbf{u} + \delta \mathbf{u})||} \\
  &\leq ||\delta \mathbf{A}|| \\
\end{align*}

\paragraph{(d)}
\begin{align*}
  \kappa &= \max_{\delta \mathbf{A}} \frac{\frac{|\delta \lambda|}{|\lambda|}}{\frac{||\delta \mathbf{A}||}{||\mathbf{A}||}}
  = \max_{\delta \mathbf{A}} \frac{|\delta \lambda|}{|\lambda|}\frac{||\mathbf{A}||}{||\delta \mathbf{A}||} \\
  &= \max_{\delta \mathbf{A}} \frac{|\delta \lambda|}{||\delta \mathbf{A}||}\frac{||\mathbf{A}||}{|\lambda|} \\
  &= \frac{||\mathbf{A}||}{|\lambda|} \geq 1\\
  \implies \kappa &\geq 1 \\
\end{align*}

\pagebreak
\section*{Problem 5}

\paragraph{(a)} Let $\lambda$ and $\mathbf{u}$ be any eigenvalue-eigenvector pair of the matrix $(\alpha \mathbf{I} - \mathbf{M})$ where $\alpha \in \mathbb{R}\\\{0\}$, then

\begin{align*}
  &(\alpha \mathbf{I} - \mathbf{M})\mathbf{u} = \lambda \mathbf{u} \\
  \implies &\mathbf{M}\mathbf{u} = (\alpha - \lambda)\mathbf{u} \\
  \implies &\mathbf{M}^2\mathbf{u} = (\alpha - \lambda)\mathbf{Mu} \\
  &\text{Since $\mathbf{M}^2 = 0$}
  \implies &\mathbf{0} = (\alpha - \lambda)^2\mathbf{Mu} \\
\end{align*}

Since $\alpha not = 0$ and $(\alpha - \lambda)^2 = 0$ for all eigen values, $\lambda = \alpha$. Which implies that the algebraic multiplicity of $\alpha$ is $n$.

\vspace{1cm}
$\boxed{\therefore \text{ True }}$

\paragraph{(b)} Let $\lambda$ and $\mathbf{u}$ be any eigenvalue-eigenvector pair of the matrix $(\alpha \mathbf{I} - \mathbf{M})$ where $\alpha \in \mathbb{R}\\\{0\}$, then

\begin{align*}
  &(\mathbf{M} - \alpha \mathbf{I})\mathbf{u} = \lambda \mathbf{u} \\
  \implies &\mathbf{M}\mathbf{u} = (\alpha + \lambda)\mathbf{u} \\
  \implies &\mathbf{M}^2\mathbf{u} = (\alpha + \lambda)\mathbf{Mu} \\
  &\text{Since $\mathbf{M}^2 = 0$}
  \implies &\mathbf{0} = (\alpha + \lambda)^2\mathbf{Mu} \\
\end{align*}

Since $\alpha not = 0$ and $(\alpha + \lambda)^2 = 0$ for all eigen values, $\lambda = -\alpha$. Which implies that the algebraic multiplicity of $-\alpha$ is $n$. Hence there is only one unique eigenvalue.

\vspace{1cm}
$\boxed{\therefore \text{ False (since k = 1)}}$

\paragraph{(c)} Let $\lambda_1, \lambda_2, \lambda_3$ and $\mathbf{u}_1,\mathbf{u}_2, \mathbf{u}_3$ be three eigenvalue-eigenvector pairs of the matirx $\mathbf{A}$ such that $\lambda_1 = \lambda_2 = \lambda_3$ and $\mathbf{u}_1, \mathbf{u}_2, \mathbf{u}_3$ are distinct.

\begin{align*}
  \implies &(\mathbf{A} - \lambda \mathbf{I})\mathbf{u}_1 = 0 \\
  &(\mathbf{A} - \lambda \mathbf{I})\mathbf{u}_2 = 0 \\
  &(\mathbf{A} - \lambda \mathbf{I})\mathbf{u}_3 = 0 \\
\end{align*}

This implies that there are $3$ vectors in the null space $\mathbf{A} - \lambda \mathbf{I}$. Hence the dimension of its null space is $3$ and the dimension of its column space is $m - 3$.

\vspace{1cm}
$\boxed{\therefore \text{ False }}$

\paragraph{(d)} Since $\mathbf{A}$ is diagonalizable, it can be written as $\mathbf{A} = \mathbf{M}\mathbf{D}\mathbf{M}^{-1}$ where $\mathbf{D}$ is a diagonal matrix such that $\mathbf{D}, \mathbf{M} \in \mathbb{R}^{m \times m}$. Let $\lambda, \mathbf{u}$ be any eigenvalue-vector pair of $\mathbf{A}$, then

\begin{align*}
  &\mathbf{Au} = \lambda\mathbf{u} \\
  \implies &\mathbf{MDM}^{-1}\mathbf{u} = \lambda\mathbf{u} \\
  \implies &\mathbf{D}(\mathbf{M}^{-1}\mathbf{u}) = \lambda (\mathbf{M}^{-1}\mathbf{u}) \\
\end{align*}

Since $\mathbf{M}$ is full rank, $\mathbf{M}^{-1}\mathbf{u}$ is unique for every $\mathbf{u}$. Hence the matrix $\mathbf{D}$ and $\mathbf{A}$ have same eigenvalues and rank and since $\mathbf{D}$ is diagonal, its eigenvalues are its diagonal elements. And since the rank of a diagonal matrix is the number of non-zero diagonal elements, rank of $\mathbf{A}$ is equal to the number of non-zero eigenvalues of $\mathbf{A}$.

\vspace{1cm}
$\boxed{\therefore \text{ True }}$

\pagebreak
\section*{Problem 6}

\paragraph{(a)} Given that, $\mathbf{A}\mathbf{w} = \mathbf{v}$
\begin{align*}
   \mathbf{v} &= (\mathbf{A} + \delta \mathbf{A})(\mathbf{w} + \delta \mathbf{w}) \\
  \implies \mathbf{A}\mathbf{w} &= (\mathbf{A} + \delta \mathbf{A})\mathbf{w}
  + (\mathbf{A} + \delta \mathbf{A})\delta \mathbf{w} \\
  \implies 0 &= \delta \mathbf{A}\mathbf{w} + (\mathbf{A} + \delta \mathbf{A}) \\
  \implies \delta \mathbf{w} &= -(\mathbf{A} + \delta \mathbf{A})^{-1}\delta \mathbf{A}\mathbf{w} \\
\end{align*}

\paragraph{(b)}
\begin{align*}
  \mathbf{v} &= a_1\mathbf{q}_1 + a_2\mathbf{q}_2 + ... + a_m\mathbf{q}_m\\
  \mathbf{w} &= \mathbf{A}^{-1}\mathbf{v} \\
  &= a_1\mathbf{A}^{-1}\mathbf{q}_1 + a_2\mathbf{A}^{-1}\mathbf{q}_2 + ... + a_m\mathbf{A}^{-1}\mathbf{q}_m\\
             &= \frac{a_1}{\lambda_1}\left[ \mathbf{q}_1 + \frac{a_2\lambda_1}{a_1\lambda_2}\mathbf{q}_{2} + ... + \frac{a_m\lambda_1}{a_1\lambda_m}\mathbf{q}_{m}\right] \\
  &\approx \frac{a_1}{\lambda_1}\mathbf{q}_1 \\
  \implies &\text{$\mathbf{w}$ is approximately in the direction of $\mathbf{q}_1$}\\
  \implies \frac{\mathbf{w}}{||\mathbf{w}||} &\approx \mathbf{q}_1\\
\end{align*}

\paragraph{(c)}
Given that $(\mathbf{A} + \delta \mathbf{A})^{-1} = \mathbf{A}^{-1} - \mathbf{A}^{-1}(\delta \mathbf{A})\mathbf{A}^{-1} + O(\epsilon_M^2)$,

\begin{align*}
  \mathbf{v} &= (\mathbf{A} + \delta \mathbf{A})(\mathbf{w} + \delta \mathbf{w}) \\
  \implies (\mathbf{A} + \delta \mathbf{A})^{-1}\mathbf{v} &= \mathbf{w} + \delta \mathbf{w} \\
  \implies (\mathbf{A}^{-1} - \mathbf{A}^{-1}(\delta \mathbf{A})\mathbf{A}^{-1} + O(\epsilon_M^2))\mathbf{Aw}
  &= \mathbf{w} + \delta \mathbf{w}\\
  \implies \mathbf{w} + \delta \mathbf{w} &\approx
  (\mathbf{I} - \mathbf{A}^{-1}(\delta \mathbf{A}))\mathbf{w} \\
  \implies \delta \mathbf{w} &\approx - \mathbf{A}^{-1}(\delta \mathbf{A})\mathbf{w} \\
\end{align*}

Since $\lambda_1$ is very less than the other eigenvalues, $||\mathbf{A}^{-1}||$ will be very high and hence $\mathbf{A}^{-1}$ will tend to extend the vector $\mathbf{w}$ in the direction of $\mathbf{q}_1$ and since $\mathbf{w}$ is already approximately in the direction of $\mathbf{q}_1$, $\delta \mathbf{w}$ is also approximately in the direction of $\mathbf{q}_1$.

Since both $\mathbf{w}$ and $\delta\mathbf{w}$ are in the direction of $\mathbf{q}_1$, $\mathbf{\tilde{w}}$ is also in the direction of $\mathbf{q}_1$. Hence $\frac{\mathbf{\tilde{w}}}{||\mathbf{\tilde{w}}||} \approx \mathbf{q}_1$.

\pagebreak
\section*{Problem 7}

\paragraph{(a)} $\mathbf{A} =
\begin{bmatrix} 
  1 & 0 & 0 \\
  1 & 2 & 0 \\
  2 & 3 & 3 \\
\end{bmatrix}$. Characteristic equation: $|\mathbf{A} - \lambda \mathbf{I}| = 0$
\begin{align*}
  \implies &(1 - \lambda)(2 - \lambda)(3 - \lambda) = 0 \\
  \implies &\lambda_1 = 1, \lambda_2 = 2, \lambda_3 = 3 \\
  &\boxed{\therefore \text{The eigenvalues are: }1, 2, 3} \\
\end{align*}

\paragraph{(b)} Given starting vector $\mathbf{b} =
\begin{bmatrix}1 \\ 0 \\ 0 \end{bmatrix}$

\begin{align*}
  \implies \mathbf{q}_1 &= \frac{\mathbf{b}}{||\mathbf{b}||} = 
  \begin{bmatrix}1 \\ 0 \\ 0 \end{bmatrix} \\
  \implies \mathbf{q}_2 &= \frac{\mathbf{v}_1}{h_{21}}
                         = \frac{\mathbf{A}\mathbf{q}_1 - \mathbf{q}_1}{||\mathbf{A}\mathbf{q}_1 - \mathbf{q}_1||} \\
&= \frac{1}{||\mathbf{A}\mathbf{q}_1 - \mathbf{q}_1||}
\begin{bmatrix} 1 & 0 & 0 \\ 1 & 2 & 0 \\ 2 & 3 & 3 \end{bmatrix}
\begin{bmatrix} 1 & 0 & 0 \end{bmatrix}
 - \begin{bmatrix} 1 & 0 & 0 \end{bmatrix}
  &= \frac{1}{\sqrt{5}}\begin{bmatrix} 1 & 1 & 2 \end{bmatrix}
\end{align*}

\paragraph{(c)} $K_2 = [\mathbf{b}|\mathbf{A}\mathbf{b}] = \mathbf{Q}_2\mathbf{R}_2$,
where $\mathbf{Q}_2 = [\mathbf{a}_1 | \mathbf{a}_2]
= \begin{bmatrix} 1 & 0 \\ 0 & 1/\sqrt{5} \\ 0 & 2/\sqrt{5} \end{bmatrix}$

\begin{align*}
  \mathbf{H}\text{(projection matrix)}  &= \mathbf{Q}_2^T\mathbf{A}\mathbf{Q}_2 \\
  &= \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1/\sqrt{5} &2/\sqrt{5} \end{bmatrix}
  \begin{bmatrix} 1 & 0 & 0 \\ 1 & 2 & 0 \\ 2 & 3 & 3 \end{bmatrix}
  \begin{bmatrix} 1 & 0 \\ 0 & 1/\sqrt{5} \\ 0 & 2/\sqrt{5} \end{bmatrix} \\
  \implies \mathbf{H} &= \begin{bmatrix} 1 & 0 \\ \sqrt{5} & 4    \end{bmatrix} \\
\end{align*}

The eigenvalues of $\mathbf{H}$ are
\begin{align*}
  &(1 - \lambda)(4 - \lambda) + 0 = 0 \\
  \implies &\lambda = 1,\,4 \\
\end{align*}

Absolute error between smallest Ritz value and smallest eigenvalue of \textbf{A} is $|1 - 1| = 0$.

Absolute error between largest Ritz value and largest eigenvalue of \textbf{A} is $|4 - 3| = 1$.

\paragraph{(d)}
\begin{align*}
  &\begin{bmatrix} 1 & 0 & 0 \\ 1 & 2 & 0 \\ 2 & 3 & 3 \end{bmatrix}
  \begin{bmatrix} x  \\ y  \\ z  \end{bmatrix}
  = \begin{bmatrix} 1  \\ 0  \\ 0  \end{bmatrix} \\
  \implies &x = 1 \\
  \implies &x + 2y = 0 \implies y = -1/2 \\
  \implies &2x + 3y + 3z = 0 \implies z = -1/6 \\
  \implies & x^{*}= \begin{bmatrix} 1  \\ -1/2  \\ -1/6  \end{bmatrix} \\
\end{align*}

Minimize $||\mathbf{A}\mathbf{c} - \mathbf{b}||$ over $\mathbf{c}$ such that $\mathbf{c} \in K_2$

Least square solution($\mathbf{\tilde{x}}$):
\begin{align*}
  \mathbf{\tilde{x}} &= \mathbf{A}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{b} \\
  \mathbf{X} &= \mathbf{A}K_2 =
  \begin{bmatrix} 1 & 0 & 0 \\ 1 & 2 & 0 \\ 2 & 3 & 3 \end{bmatrix}
  \begin{bmatrix} 1 & 1 \\ 0 & 1 \\ 0 & 2 \end{bmatrix} \\
  &=\begin{bmatrix} 1 & 1 \\ 1 & 3 \\ 2 & 11 \end{bmatrix} \\
  \implies \mathbf{\tilde{x}} &=
  \mathbf{A}\begin{bmatrix} 6 & 26 \\ 26 & 131 \end{bmatrix}^{-1}
  \begin{bmatrix} 1 \\ 1 \end{bmatrix} \\
  &= \begin{bmatrix} 1 & 0 & 0 \\ 1 & 2 & 0 \\ 2 & 3 & 3 \end{bmatrix}
  \begin{bmatrix} 0.95 \\ -0.18 \end{bmatrix} \\
  \therefore \mathbf{\tilde{x}} &= \begin{bmatrix} 0.77 \\ -0.18 \\ -0.37 \end{bmatrix} \\
\end{align*}

2 Norm of error between $\mathbf{x}^{*}$ and $\mathbf{\tilde{x}}$ is
\begin{align*}
  \left\Vert \begin{bmatrix} 1  \\ -1/2  \\ -1/6  \end{bmatrix}
  - \begin{bmatrix} 0.77 \\ -0.18 \\ -0.37 \end{bmatrix}\right\Vert
  = 0.44
\end{align*}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
