{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0jZ27oRYMqM"
      },
      "source": [
        "# Question 1 : Logistic Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGfDsfDFYMqN"
      },
      "source": [
        "## Import functions and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "862oT5OZYMqO"
      },
      "outputs": [],
      "source": [
        "# run this cell to import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from os import getcwd\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyc7xO5PYMqO"
      },
      "source": [
        "### Imported functions\n",
        "\n",
        "Download the data needed for this assignment. Check out the [documentation for the twitter_samples dataset](http://www.nltk.org/howto/twitter.html).\n",
        "\n",
        "* twitter_samples and stopwords: While running on a local computer you need to download them using\n",
        "```Python\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "```\n",
        "\n",
        "#### Import some helper functions that we provided in the utils.py file:\n",
        "* `clean_tweet()`: cleans, tokenizes, removes stopwords, and converts words to stems.\n",
        "* `build_frequency()`: this counts how often a word in the the entire set dataset of tweets was associated with a positive label '1' or a negative label '0', then builds the `frequency_words` dictionary, where each key is a (word,label) tuple, and the value is the count of its frequency within the corpus of tweets.\n",
        "* The `frequency_words` dictionary is the frequency dictionary that's being built. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3sygxTKYMqP",
        "outputId": "edaddf83-3c82-4ad7-87e8-f3f64ee49a9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lyj3-HutYMqP"
      },
      "outputs": [],
      "source": [
        "# this code allows us to prevent downloading data again while refreshing our workspace\n",
        "filePath = f\"{getcwd()}/../temp/\"\n",
        "nltk.data.path.append(filePath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReJSl_kgYMqQ",
        "outputId": "6c9bcf20-7cab-413b-8977-58ff9a92d57d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/../temp/\n"
          ]
        }
      ],
      "source": [
        "print(filePath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7L3q347YMqQ"
      },
      "source": [
        "### Data processing\n",
        "* The `twitter_samples` contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "vRgnF6oeYMqQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.corpus import twitter_samples \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hPfRZL0UYMqQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    \n",
        "    # tweets_clean: a list of words containing the processed tweet\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market symbols like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in string.punctuation and  \n",
        "                word not in stopwords_english): \n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean\n",
        "\n",
        "\n",
        "def build_frequency(tweets, y_np):\n",
        " \n",
        "    # Convert np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    yslist = np.squeeze(y_np).tolist()\n",
        "\n",
        "    # Start with an empty dictionary and populate it by looping over all tweets\n",
        "    # and over all processed words in each tweet.\n",
        "    freqs = {}\n",
        "    for i in range(len(tweets)):\n",
        "        tweet = tweets[i]\n",
        "        y = yslist[i]\n",
        "        for word in clean_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xLCdJXUNYMqR"
      },
      "outputs": [],
      "source": [
        "# select the set of positive and negative tweets\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdW2m7pvYMqR",
        "outputId": "4c80c226-4f45-419d-b88a-30a00c4621ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Impatientraider On second thought, thereâ€™s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.\n",
            "I have a really good m&amp;g idea but I'm never going to meet them :(((\n"
          ]
        }
      ],
      "source": [
        "print(positive_tweets[7])\n",
        "print(negative_tweets[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI006cOLYMqS"
      },
      "source": [
        "### Feature Extraction\n",
        "\n",
        "* Given a list of tweets, extract the features and store them in a matrix. You will extract two features.\n",
        "    * The first feature is the number of positive words in a tweet.\n",
        "    * The second feature is the number of negative words in a tweet. \n",
        "    * For each word, check the `frequency_words` dictionary for the count when that word has a positive '1' label. (Check for the key (word, 1.0)\n",
        "    * Do the same for the count for when the word is associated with the negative label '0'. (Check for the key (word, 0.0).)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "bx_HWFabYMqS"
      },
      "outputs": [],
      "source": [
        "def extract_features(tweet, freqs):\n",
        "    \n",
        "    # clean_tweet tokenizes, stems, and removes stopwords\n",
        "    word_l = clean_tweet(tweet)\n",
        "    \n",
        "    # 3 elements in the form of a 1 x 3 vector\n",
        "    x = np.zeros((1, 3)) \n",
        "    x[0,0] = 1 \n",
        "    \n",
        "    for word in word_l:\n",
        "        \n",
        "        # increment the word count when the  label is positive \n",
        "        x[0,1] = x[0,1] + freqs.get((word, 1.0),0)\n",
        "        # increment the word count when the  label is negative \n",
        "        x[0,2] = x[0,2] + freqs.get((word, 0.0),0)\n",
        "        \n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqwbKYnqYMqS"
      },
      "source": [
        "#### Instructions: Write `sigmoid`\n",
        "Finds the sigmoid of z "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "d7UdRNDKYMqS"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z): \n",
        "    \n",
        "    # z is an input which can be a scalar or an array and h is the sigmoid of z \n",
        "    # write the formula for sigmoid here and assign it to h\n",
        "    h = 0\n",
        "    return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tko2FX1RYMqS"
      },
      "source": [
        "#### Instructions: Write `predict_positivity_score`\n",
        "Predict whether a tweet is positive or negative.\n",
        "\n",
        "* Given a tweet, process it, then extract the features.\n",
        "* Apply the model's learned weights on the features to get the y.\n",
        "* Apply the sigmoid to the y to get the prediction (a value between 0 and 1).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "xJ-27hS8YMqT"
      },
      "outputs": [],
      "source": [
        "def predict_positivity_score(tweet, freqs, theta):\n",
        "    \n",
        "    \n",
        "    # extracting features from tweet and the frequencies, this x will multiply with the coefficients which are passed to the \n",
        "    # sigmoid \n",
        "    x = extract_features(tweet,freqs)\n",
        "    \n",
        "    # make the prediction using x and theta\n",
        "    # you need to make calculations for y_pred here. You may need to call sigmoid function here\n",
        "    y_pred = 0\n",
        "    \n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF-l_R5-YMqT"
      },
      "source": [
        "Note that the `frequency_words` dictionary should be based on the training data and training labels. Here we have done this for a few number of data points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfycpLqNYMqT"
      },
      "source": [
        "\n",
        "The given function `clean_tweet()` makes tokens from words and applies stemming (producing some variant of a root/base word) and removes stop words (commonly used words such as \"the\" ,\"a\" ,\"an\" among other words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_8xTSni1YMqT"
      },
      "outputs": [],
      "source": [
        "#IMPLEMENT gradient descient here. \n",
        "# alpha is the learning rate \n",
        "# x is the data and y is hte label \n",
        "# theta is the initial parameter values \n",
        "# num_iters is the number of iterations you want the algorithm to run\n",
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    \n",
        "    # list_of_loss_values is the loss for each iteration which , same is the case with training accuracy\n",
        "    return J, theta,list_of_loss_values,list_of_training_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw8LMm3tYMqT"
      },
      "source": [
        "* Train test split: 25% will be in the test set, and 75% in the training set.\n",
        "\n",
        "# Example\n",
        " # Here we show how to call these methods for a few data points. You  may have to use similar calls to the training data after you make the test train split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "YxVoQKXWYMqT"
      },
      "outputs": [],
      "source": [
        "some_number_of_tweets = positive_tweets[0:10] + negative_tweets[0:10]\n",
        "some_number_of_labels = np.append(np.ones((len(positive_tweets[0:10]), 1)), np.zeros((len(negative_tweets[0:10]), 1)), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6EOTWnxjYMqT"
      },
      "outputs": [],
      "source": [
        "frequency_words = build_frequency(some_number_of_tweets, some_number_of_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X37PhtIiYMqU",
        "outputId": "4b4259ff-280f-4fb4-8f30-f58f5bb67727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1. 19.  0.]]\n",
            "[[1. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Random tests, you can remove these if you want later , but it may help in testing the code \n",
        "print(extract_features(some_number_of_tweets[6], frequency_words))\n",
        "# test 2:\n",
        "# check for when the words are not in the frequency_words dictionary\n",
        "print(extract_features('lalalalala blahblahblah bobobobobbob', frequency_words))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "opIE_J2bYMqU"
      },
      "outputs": [],
      "source": [
        "# NOTE : call gradient descent to get coefficents and then pass that coefficents into predict function \n",
        "# something like : predict_positivity_score(tweet, frequency_words, coefficents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hha87quYMqU"
      },
      "source": [
        "## Testing your model\n",
        "After training your model using the training set above, check how your model might perform on real, unseen data, by testing it against the test set.\n",
        "\n",
        "#### Implement `test_logistic` \n",
        "* Given the test data and the weights of your trained model, calculate the accuracy of your logistic regression model. \n",
        "* Use your `predict_positivity_score()` function to make predictions on each tweet in the test set.\n",
        "* If the prediction is >= 0.5, set the model's the predicted label is 1 otherwise it is 0.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "MPPVlhEaYMqU"
      },
      "outputs": [],
      "source": [
        "# Testing your model on the test set\n",
        "def test_logistic(test_x, test_y, freqs, theta):\n",
        "    \n",
        "    #use your trained model to make predictions and then compare those predictions with the \n",
        "    # actual values to come up with an accuracy. and return this accuracy\n",
        "    \n",
        "    accuracy = 0\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ebh08KlSYMqU"
      },
      "outputs": [],
      "source": [
        "# Use your model to predict what these result in, whether it is a positive or negative sentiment. If possible, feel free to give\n",
        "# an intuitive explanation (short explanation) of the scores that you obtained\n",
        "\n",
        "my_tweet = ['Let that sink in',\n",
        "            'My psychiatrist told me I was crazy and I said I want a second opion. He said okay, you are ugly too ',\n",
        "            'Iâ€™d rather have a drink with Mel Gibson in his hotel tonight than Bill Cosby.',\n",
        "            'Building trust is the key to success in any relationship. Excuses, irregularity, chronically late, etc., are the ingredients to kill the TRUST.',\n",
        "            'We are best friends. Always remember that if you fall i will pick you up. After I finish laughing'\n",
        "           ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8SGyU3AEYMqa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ZKgBI9MJYMqa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}